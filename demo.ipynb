{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c457f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from label_generation import generate_labels, save_generation_output\n",
    "from label_clustering import cluster_labels_gpt, make_clustering_prompt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bbff1f",
   "metadata": {},
   "source": [
    "## 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"jhu-clsp/astro-llms-full-query-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary where keys are thread id and values are full user query\n",
    "data_processed = {}\n",
    "label_gold = {}\n",
    "full_data = True\n",
    "n_samples = len(ds['train']) if full_data else 25\n",
    "for i in range(n_samples):\n",
    "    segment_id = 0 # some datasets have multiple segments per document, here we only use 0 as there is only one segment\n",
    "    thread_id = str(ds['train'][i]['thread_ts']) + '_' + str(segment_id)\n",
    "    data_processed[thread_id] = ds['train'][i]['full_user_query']\n",
    "    label_gold[thread_id] = [ds['train'][i]['Open Coding']]\n",
    "# print first 3 items\n",
    "print(\"Examples of processed data:\")\n",
    "list(data_processed.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47915795",
   "metadata": {},
   "source": [
    "## 2. Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53727bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for pipeline\n",
    "config = {\n",
    "# Generation Model \n",
    "\"model_name\": \"gpt-4o-mini\",\n",
    "\n",
    "# Generation\n",
    "\"generation_output_dir\": \"./results/generation\", # Change Me\n",
    "\n",
    "# CLustering\n",
    "\"cluster_model_name\": \"gpt-4o-mini\",\n",
    "\"cluster_output_dir\": \"./results/clustering\" # Change Me, set to current directory for demo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for label generation\n",
    "background = \"\"\"\n",
    "Query type refers to the strategy, motivation or knowledge solicited by the user. It is NOT about the topic of the query content. \n",
    "An LLM-powered bot is deployed for scientists to query literature in astronomy and then to analyze scientists' initial interactions. \n",
    "\"\"\"\n",
    "\n",
    "coding_goal = \"understanding the query type to the literature search bot from the astronomy scientists.\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "{background}\n",
    "\n",
    "We are using the queries to this bot to conduct INDUCTIVE Coding. The labeling aims to {coding_goal}\n",
    "\n",
    "Instruction:\n",
    "- Label the input only when it is HIGHLY RELEVANT and USEFUL for {coding_goal}.\n",
    "- Then, define the phrase of the label. The label description should be observational, concise and clear.\n",
    "- ONLY output the label and DO NOT output any explanation.\n",
    "\n",
    "Format:\n",
    "- Define the label using the format \\\"LABEL: [The phrase of the label]\\\". \n",
    "- If there are multiple labels, each label is a new line. \n",
    "- If the input is irrelevant, use \\\"LABEL: [Irrelevant]\\\". \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13008783",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_result = generate_labels(data_processed, system_prompt, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save generation results\n",
    "gen_result_id = save_generation_output(gen_result, config[\"generation_output_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4dc534",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ca94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"astrobot\"\n",
    "cluster_prompt = make_clustering_prompt(dataset=dataset)\n",
    "cluster_result = cluster_labels_gpt(gen_result, cluster_prompt, config, gen_result_id=gen_result_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6ecf0",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_themes = list(set([i.lower() for i in ds['train']['Open Coding']]))\n",
    "pred_themes = list(set([i.lower() for i in cluster_result[-1].keys()]))\n",
    "print(\"==== Gold Themes ====\")\n",
    "for i in gold_themes:\n",
    "\tprint(i)\n",
    "print(\"==== Predicted Themes ====\")\n",
    "for i in pred_themes:\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2087481",
   "metadata": {},
   "source": [
    "### Theme precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13330579",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.4 # You can adjust this threshold based on your needs\n",
    "theme_prec_score = theme_precision(gold_themes, pred_themes, cos_sim_thresh=similarity_threshold)\n",
    "theme_recall_score = theme_recall(gold_themes, pred_themes, cos_sim_thresh=similarity_threshold)\n",
    "print(f\"Theme Precision: {theme_prec_score:.2f}\")\n",
    "print(f\"Theme Recall: {theme_recall_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12348a",
   "metadata": {},
   "source": [
    "### Segment Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to load previous generation results, you can do so as follows:\n",
    "# gen_result_id = \"<your_generation_result_id>\" # Provide your generation result ID here\n",
    "# gen_result = f\"./results/generation/generation_{gen_result_id}.json\"\n",
    "# gen_result = json.load(open(gen_result, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86299",
   "metadata": {},
   "source": [
    "#### Mapping themes to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_dir = f\"./results/clustering/clustering_{gen_result_id}\"\n",
    "cluster_mapping = create_mapping(clustering_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e440882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add final themes from mapping to gen_result\n",
    "for doc_id in gen_result.keys():\n",
    "    for segment in gen_result[doc_id][\"LLM_Annotation\"]:\n",
    "        for label in segment[\"label\"]:\n",
    "            theme = cluster_mapping.get(label.lower(), None)\n",
    "            if theme is not None:\n",
    "                segment.setdefault(\"theme\", set()).add(theme)\n",
    "            else:\n",
    "                segment.setdefault(\"theme\", set()).add(\"irrelevant\")\n",
    "        segment[\"theme\"] = list(segment[\"theme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3814ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gold labels to gen_result\n",
    "for k in gen_result.keys():\n",
    "    for i, segment in enumerate(gen_result[k][\"LLM_Annotation\"]):\n",
    "        segment[\"gold_label\"] = label_gold[k+\"_\"+str(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88337112",
   "metadata": {},
   "source": [
    "#### Evaluate segment-level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.4 # You can adjust this threshold based on your needs\n",
    "prec_by_theme = get_precision_by_theme(gold_themes, pred_themes, gen_result, similarity_threshold=similarity_threshold)\n",
    "seg_prec_score = segment_precision(prec_by_theme)\n",
    "recall_by_theme = get_recall_by_theme(gold_themes, pred_themes, gen_result, similarity_threshold=similarity_threshold)\n",
    "seg_recall_score = segment_recall(recall_by_theme)\n",
    "print(f\"Segment Precision: {seg_prec_score:.2f}\")\n",
    "print(f\"Segment Recall: {seg_recall_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
